{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c533ea7b-b3c7-4a65-a4f8-83a21182e918",
   "metadata": {},
   "source": [
    "[<< Previous: Data Exploration](02_data_explore.ipynb) &nbsp; | &nbsp; [Next: Fit Analysis >>](04_fit_analysis.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dce0876-eeb9-4657-820f-8f84e0fa8e94",
   "metadata": {},
   "source": [
    "# Model Fitting\n",
    "\n",
    "In this notebook, we will perform model fitting and collect metrics for evaluating the performance of our fitted model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280fe7a3-718a-41a9-ae84-91ac0e58bc17",
   "metadata": {},
   "source": [
    "First, let's import the libraries that will be required for this notebook.\n",
    "\n",
    "We will use code from our custom package for this project, `myutilpy`. Specifically, we will utilize model implementations and utility functions from the `myutilpy.models` module. All of the source code is available in this repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19bf0c32-abf2-4170-8b46-93288e4fc514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import yaml\n",
    "import multiprocessing\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import lightning.pytorch as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from lightning.pytorch.callbacks import RichProgressBar, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import TensorBoardLogger, CSVLogger\n",
    "from lightning.pytorch.utilities.model_summary import ModelSummary\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from myutilpy.models.text_regressor import TextRegressor, LitTextRegressor\n",
    "from myutilpy.models.pooling import pooling_fns, pool_cls, pool_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5092940b-733c-4dd5-8df2-19ab3b985bca",
   "metadata": {},
   "source": [
    "Before we move on, let's silence a few unconsequential, known warning messages that will clutter our cell output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a233a63a-b9b9-43a4-845e-bcf963c1a3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*Detected KeyboardInterrupt, attempting graceful shutdown.*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*Only `best_model_path` will be reloaded.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989fb736-28e5-45c0-9ebf-be86c9c3ec6e",
   "metadata": {},
   "source": [
    "## Configurations\r\n",
    "\r\n",
    "Next, let’s do some setup. We will load the associated configurations for the desired experiment.\r\n",
    "\r\n",
    "We are fine-tuning a [`MiniLM-L6`](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) model. This model generates embeddings that capture information about passages of text, and can be used for various NLP tasks. This model is much lighter-weight (i.e., has fewer parameters) than other transformer models well-suited to the same task, but still delivers good quality performance. A helpful comparison of model architecture performance can be found [here](https://www.sbert.net/docs/pretrained_models.html). This comparison was created by the authors of this (and several other) models uploaded to the [Hugging Face](https://huggingface.co/) model repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b868704-6b23-4a24-84a6-fe4077833f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_id = \"mlml6_rate_pred_clsp\"\n",
    "num_cores_avail = max(1, multiprocessing.cpu_count() - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2993eeb5-c0e8-4d4a-a1e2-72ff2042e572",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../experiments/configs/{config_id}/main.yaml\", 'r') as f:\n",
    "    main_config = yaml.safe_load(f)\n",
    "\n",
    "with open(f\"../experiments/configs/{config_id}/model.yaml\", 'r') as f:\n",
    "    model_config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "714dcbe9-12ca-468f-96d4-231667e559a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_checkpoint = main_config[\"dataset_checkpoint\"]\n",
    "dataset_checkpoint_revision = main_config[\"dataset_checkpoint_revision\"]\n",
    "pt_model_checkpoint = main_config[\"pt_model_checkpoint\"]\n",
    "pt_model_checkpoint_revision = main_config[\"pt_model_checkpoint_revision\"]\n",
    "dataset_id = main_config[\"dataset_id\"]\n",
    "frozen_model_checkpoint_path = model_config[\"frozen_model_checkpoint_path\"]\n",
    "finetune_model_checkpoint_path = model_config[\"finetune_model_checkpoint_path\"]\n",
    "model_seed = model_config[\"model_seed\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c493fef-aecc-4957-bb8b-6d9436ca0c92",
   "metadata": {},
   "source": [
    "## Base model, tokenizer, and dataset\n",
    "\n",
    "Let's load in our base embedding model, tokenizer, and preprocessed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d42f506-1222-4a62-bba7-7717ceb99f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = AutoModel.from_pretrained(\n",
    "    pt_model_checkpoint,\n",
    "    revision=pt_model_checkpoint_revision\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pt_model_checkpoint,\n",
    "    revision=pt_model_checkpoint_revision\n",
    ")\n",
    "\n",
    "datasets = load_from_disk(f\"../data/pitchfork/{dataset_id}/dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72467087-5a5e-4565-9c2f-9593441d9a8c",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "We only need a subset of the dataset columns for our model-fitting. Our tokenizer will output the `\"input_ids\"` and `\"attention_mask\"` columns. Strictly speaking, we only really need the `\"input_ids\"`, `\"attention_mask\"`, and `\"rating\"` columns, but keeping additional columns can be helpful for debugging and development purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61e5339d-c556-45e5-b547-6916170f6fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "keeper_cols = [\"artist\", \"album\", \"year_released\", \"rating\", \"input_ids\", \"attention_mask\"]\n",
    "drop_cols = set(datasets[\"train\"].column_names).difference(set(keeper_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "993f416c-d209-47e3-a701-f33082f1857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = (\n",
    "    datasets\n",
    "        .map(lambda examples: tokenizer(examples[\"review\"], padding=True, truncation=True), batched=True, num_proc=num_cores_avail)\n",
    "        .remove_columns(drop_cols)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ccd385-8f9b-421d-b70c-757df8f44661",
   "metadata": {},
   "source": [
    "### DataLoaders\r\n",
    "\r\n",
    "Let's set up the `DataLoader` objects that will be used for fitting and evaluating our mode.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1424a9b1-2395-4c7e-b3a1-eae07670679f",
   "metadata": {},
   "source": [
    "Our first task will be to define a collation function whose job it is to organize our batched examples into tensors that are ready to be passed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2d26608-f5da-4be4-bfa4-f8c384ed2fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_reviews(batch):\n",
    "    # Extract input_ids and labels from the batch\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_masks = [item['attention_mask'] for item in batch]\n",
    "    ratings = [item['rating'] for item in batch]\n",
    "\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "    ratings = torch.tensor(ratings)\n",
    "\n",
    "    return input_ids, attention_masks, ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437dbd76-3615-4380-9db5-359de24d4aad",
   "metadata": {},
   "source": [
    "Next, let's instantiate the `DataLoader` objects. Notice that we have some commented-out code. We also define our batch size, since this is required to instantiate a `DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "749908e3-02d6-4aa8-9834-5379af5959ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f109e655-fe2e-42a9-bb6f-c5c84a0fb80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], batch_size=batch_size, collate_fn=collate_reviews, shuffle=True)\n",
    "valid_dataloader = DataLoader(tokenized_datasets[\"validation\"], batch_size=batch_size, collate_fn=collate_reviews)\n",
    "test_dataloader = DataLoader(tokenized_datasets[\"test\"], batch_size=batch_size, collate_fn=collate_reviews)\n",
    "\n",
    "# # Random subsets for quick development\n",
    "# train_dataloader = DataLoader(tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1500)), batch_size=batch_size, collate_fn=collate_reviews, shuffle=True)\n",
    "# valid_dataloader = DataLoader(tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(1500)), batch_size=batch_size, collate_fn=collate_reviews)\n",
    "# test_dataloader = DataLoader(tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1500)), batch_size=batch_size, collate_fn=collate_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581f7860-a17a-4063-8819-70365d024128",
   "metadata": {},
   "source": [
    "Before moving on, let's do a quick spot check to make sure that our forward pass and pooling code are outputting tensors of correct dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1785ce37-73af-4261-81c8-8ec4e63c8a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, batch in enumerate(valid_dataloader):\n",
    "    input_ids, attention_masks, ratings = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6929f59d-2b5d-4c71-9daf-c629ccbce19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    embedding = embedding_model(input_ids=input_ids, attention_mask=attention_masks).last_hidden_state\n",
    "mp_embedding = pool_mean(embedding, attention_masks)\n",
    "cp_embedding = pool_cls(embedding, attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "607e4c9f-2cf1-4bfe-9df8-efe19d909028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 384\n",
      "16 384\n",
      "16 384\n"
     ]
    }
   ],
   "source": [
    "print(batch_size, embedding_model.config.hidden_size)\n",
    "print(*mp_embedding.shape)\n",
    "print(*cp_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37d13a1-be23-43a6-bb12-8cfc0317e3ce",
   "metadata": {},
   "source": [
    "Everything looks good in terms of dimensionality. Our pooled embeddings are outputting tensors of shape (batch size, embedding dimension)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd5498b-9244-4257-b08f-379977f06292",
   "metadata": {},
   "source": [
    "## Setup for fitting\r\n",
    "\r\n",
    "Now, let's prepare to fit our model. Notice that we have two epoch variables. When fine-tuning pre-trained models, it is common practice to\r\n",
    "\r\n",
    "1. freeze the weights of the base model and train only the task-specific model head for a round of training, then\r\n",
    "2. select the best-performing parameters the \"frozen\" roundround of training, un-freeze the base model parameters, and do a second round of training of all model parame A relatively small learning rate is usually used for this stage.ters.\r\n",
    "\r\n",
    "Here, we will perform very few epochs of training in both stages for a couple of reasons. First, this code is not being executed in a cloud environment with large amounts of compute, so we will try and keep things manageable. Second, given the dataset size and the relatively small number of parameters in the regression model head, the first step likely does not need all that many epochs to obtain good performance. Finally, when fine-tuning the model, it is often the case that overfitting will occur within just a few epochs due to how powerful the base model will typically be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9c74bc-2cad-4313-a4bc-1a01d9070d57",
   "metadata": {},
   "source": [
    "Let's define our epoch counts and also define the device that our model will run on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9dbe54ec-f19e-41a1-ae57-27b9e36897b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "frozen_epochs = 20\n",
    "finetune_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173f681e-1ec2-4357-816a-dd9d493b1099",
   "metadata": {},
   "source": [
    "Let's also set a random seed for model fitting so that we can reproduce our results if need be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9176615b-cb46-4190-b1df-319478e53e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(seed=model_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01efb133-6a19-4761-91e8-53cfc06ddb0b",
   "metadata": {},
   "source": [
    "## Frozen fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1edbf9-0322-4e3c-b20f-4150e7923210",
   "metadata": {},
   "source": [
    "### Frozen trainer\r\n",
    "\r\n",
    "For this project, we will be using the [`Lightning`](https://lightning.ai/) library. It provides many helpful utilities and takes away a lot of the boilerplate programming work that can slow down model development in `PyTorch`. For details on how `Lightning` is used to help with our model fitting and evaluation code, check out the `myutilpy.models.text_regressor` module in the source code. For the purposes of this notebook, we only really need to set up the `Trainer` object. The trainer will handle model training and evaluation for us during the fitting process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0b5706-8b8f-477c-ae0c-90d78ebe3e50",
   "metadata": {},
   "source": [
    "The first thing we will take care of for setting up the trainer is to define our logging and checkpointing utilities. We will log metrics to both a `.csv` file and for `tensorboard` visualization. While the tensorboard output will not appear in this notebook's output, it is a very convenient tool for examining model behavior during fitting and for comparing fitting behavior across models or versions of the same model. The `tensorboard` output was used during development of these notebooks and the code to utilize it is retained for those who are interested in replicating the results. We also want to make sure we retain checkpoints of our model so that we can load them in later. Here, we decide to keep only the checkpoint whose model parameters yielded the minimum validation loss out of all training epochs. Notice that we specify this checkpointer is only for the \"frozen\" training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d9cd192-3269-4541-a0fe-44f17e0150f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_base = f\"../experiments/results/{config_id}\"\n",
    "\n",
    "csv_logger = CSVLogger(results_base, \"frozen_lightning_logs\")\n",
    "tb_logger = TensorBoardLogger(results_base, name=\"frozen_tb_logs\")\n",
    "frozen_model_checkpointer = ModelCheckpoint(\n",
    "    f\"{results_base}/frozen_checkpoints/version_{csv_logger.version}\",\n",
    "    filename=\"checkpoint\",\n",
    "    monitor=\"avg_val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=1\n",
    ")\n",
    "\n",
    "loggers = [csv_logger, tb_logger]\n",
    "callbacks = [frozen_model_checkpointer, RichProgressBar()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3aec3f-fb6c-412a-8365-65ebc493e4d4",
   "metadata": {},
   "source": [
    "Our next step is to instantiate our frozen `Trainer` object. Notice that we also make a (potential) modification to the number of frozen training epochs. The reason for this is that a saved checkpoint will contain the epoch at which the checkpoint was written out. Since this notebook was already run (behind the scenes, like a cooking show where all of the ingredients are always magically prepped!) and the best version of the frozen model was already saved out, we don't want to start training where we left off if it just so happens that the best version of the model was found before the final epoch. This would be the case if, since we only saved the best model version in terms of validation loss, overfitting began to occur. So, we update our `max_epochs` to reflect the number of epochs it took to find the best frozen model (behind the scenes) and this means the trainer will not attempt to run additional fitting epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13f80ce5-0fcf-4704-b35a-6ef5d423a0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "if frozen_model_checkpoint_path is not None:\n",
    "    checkpoint = torch.load(f\"../{frozen_model_checkpoint_path}\")\n",
    "    # Account for zero indexing\n",
    "    frozen_epochs = checkpoint[\"epoch\"] + 1\n",
    "\n",
    "frozen_trainer = pl.Trainer(\n",
    "    max_epochs=frozen_epochs,\n",
    "    accelerator=accelerator,\n",
    "    callbacks=callbacks,\n",
    "    precision=\"16-mixed\",\n",
    "    logger=loggers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53e6ba1-dbb7-4424-b338-bdbbd681964c",
   "metadata": {},
   "source": [
    "### Model\r\n",
    "\r\n",
    "Let's now instantiate our full model (base model + regression head) using the `TextRegressor` class defined in `myutilpy.models.text_regressor\n",
    "\n",
    "Note that the pooling method we use for this experiment is \"CLS pooling\" (pooling by extracting the starting `[CLS]`) token. This is a popular method when using BERT-based models.`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce37ecf8-030c-4e97-b49c-6917e4732070",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_model = TextRegressor(\n",
    "    embedding_model,\n",
    "    embed_dim=embedding_model.config.hidden_size,\n",
    "    pooling_fn=pooling_fns[model_config[\"pooling\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7986462e-ab29-4a34-96ac-dcbabdcdeb06",
   "metadata": {},
   "source": [
    "### Freeze parameters\n",
    "\n",
    "The final step is to instantiate our `Lightning` model `LitTextRegressor` which wraps around our `TextRegressor` model and freeze the base model parameters. A helper method `LitTextRegressor.freeze_pretrained_model()` has been implemented to freeze the base model parameters and set the learning rate appropriately for frozen training. For full implementation details, see `myutilpy.models.text_regressor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea3f7f87-1f08-4f9f-b8a3-803786c0c4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model = LitTextRegressor(tr_model)\n",
    "lit_model.freeze_pretrained_model(lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558d2f7e-7690-4459-8134-a2b70fe64561",
   "metadata": {},
   "source": [
    "Let's verify that the base model parameters are indeed frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4220bf6a-4fc5-4aea-8679-a1f9aa416e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  | Name                           | Type          | Params\n",
       "-----------------------------------------------------------------\n",
       "0 | text_regressor                 | TextRegressor | 22.7 M\n",
       "1 | text_regressor.embedder        | BertModel     | 22.7 M\n",
       "2 | text_regressor.regression_head | Linear        | 385   \n",
       "-----------------------------------------------------------------\n",
       "385       Trainable params\n",
       "22.7 M    Non-trainable params\n",
       "22.7 M    Total params\n",
       "90.854    Total estimated model params size (MB)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ModelSummary(lit_model, max_depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf71bf6-b881-44c5-9151-d6c8c6a35997",
   "metadata": {},
   "source": [
    "### Fitting the frozen model\n",
    "\n",
    "Finally, we fit our model with base parameters frozen. Again, because the model was already fit behind the scenes, a checkpoint path is used. Apologies for being denied the satisfaction of observing training bars that have reached completion &#x1F641;."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8da17839-011e-478c-9ecd-9bfb3ea22269",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at ../experiments/results/mlml6_rate_pred_clsp/frozen_checkpoints/version_0/checkpoint.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from: experiments/results/mlml6_rate_pred_clsp/frozen_checkpoints/version_0/checkpoint.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name           </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type          </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ text_regressor │ TextRegressor │ 22.7 M │\n",
       "└───┴────────────────┴───────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName          \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType         \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ text_regressor │ TextRegressor │ 22.7 M │\n",
       "└───┴────────────────┴───────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 385                                                                                              \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 22.7 M                                                                                       \n",
       "<span style=\"font-weight: bold\">Total params</span>: 22.7 M                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 90                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 385                                                                                              \n",
       "\u001b[1mNon-trainable params\u001b[0m: 22.7 M                                                                                       \n",
       "\u001b[1mTotal params\u001b[0m: 22.7 M                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 90                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restored all states from the checkpoint at ../experiments/results/mlml6_rate_pred_clsp/frozen_checkpoints/version_0/checkpoint.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77000e2626924f5a9f3225b77d123ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if frozen_model_checkpoint_path is not None:\n",
    "    print(f\"Loading checkpoint from: {frozen_model_checkpoint_path}\")\n",
    "    frozen_trainer.fit(\n",
    "        model=lit_model,\n",
    "        ckpt_path=f\"../{frozen_model_checkpoint_path}\",\n",
    "        train_dataloaders=train_dataloader,\n",
    "        val_dataloaders=valid_dataloader\n",
    "    )\n",
    "else:\n",
    "    print(f\"Training from scratch\")\n",
    "    frozen_trainer.fit(\n",
    "        model=lit_model,\n",
    "        train_dataloaders=train_dataloader,\n",
    "        val_dataloaders=valid_dataloader\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9bb178-525b-4c5e-bd6a-ef3891d08f24",
   "metadata": {},
   "source": [
    "Before we move on to the next stage, let's see how well the best version of our frozen model performs on our validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44c549b7-7e01-4c97-b757-4f9598d53664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/carcook/dev/nlp-projects/experiments/results/mlml6_rate_pred_clsp/frozen_checkpoints/version_0/checkpoint.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /home/carcook/dev/nlp-projects/experiments/results/mlml6_rate_pred_clsp/frozen_checkpoints/version_0/checkpoint.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5be383f0d2e340b9b72f821905a95146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       avg_test_loss       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1630005836486816     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      avg_test_loss      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1630005836486816    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'avg_test_loss': 1.1630005836486816}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_frozen_checkpoint_path = frozen_trainer.checkpoint_callback.best_model_path\n",
    "frozen_trainer.test(lit_model, dataloaders=valid_dataloader, ckpt_path=best_frozen_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6fa35286-4179-45dd-810e-b4be84abc93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mse': 1.1630003, 'rmse': 1.0784249}\n"
     ]
    }
   ],
   "source": [
    "print(lit_model.test_epoch_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b39a068-0f14-46be-bbb9-94f60801d350",
   "metadata": {},
   "source": [
    "Pretty good! Using a fully frozen pre-trained base model, we have trained a regression model head to predict ratings to within +/- 1.1 rating \"points\" based only on review text (on our validation set)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d61b36-2ea3-4022-9a95-7ea2dcbf6613",
   "metadata": {},
   "source": [
    "## Un-frozen fitting (fine-tuning)\r\n",
    "\r\n",
    "It is now time for the second stage, where we un-freeze our base model's parameters and perform fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6be3a3-896c-4aec-8e29-c74540b6ae79",
   "metadata": {},
   "source": [
    "Before we move on, let's clear up some memory on our GPU (if applicable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28e3797f-dd59-4446-a81b-a1dce8fa6e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free GPU memory\n",
    "del lit_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab08990-4d1c-4656-a391-62aee56d0a4e",
   "metadata": {},
   "source": [
    "### Load in best frozen model\r\n",
    "\r\n",
    "Let's load in the best checkpoint for our frozen model and use it in the fine-tuning stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "278f34ab-4c43-4647-9107-91abaa5e2936",
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model = LitTextRegressor.load_from_checkpoint(\n",
    "    frozen_trainer.checkpoint_callback.best_model_path,\n",
    "    text_regressor = tr_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc00a9e-324b-4be2-9f64-e9dda389c6fa",
   "metadata": {},
   "source": [
    "Let's now un-freeze the base model's parameters (and set a lower learning rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "03e71598-93db-4717-a02b-042f9a3cd35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model.unfreeze_pretrained_model(1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2163f39c-5502-44b4-b7b7-04c9594c02a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  | Name                           | Type          | Params\n",
       "-----------------------------------------------------------------\n",
       "0 | text_regressor                 | TextRegressor | 22.7 M\n",
       "1 | text_regressor.embedder        | BertModel     | 22.7 M\n",
       "2 | text_regressor.regression_head | Linear        | 385   \n",
       "-----------------------------------------------------------------\n",
       "22.7 M    Trainable params\n",
       "0         Non-trainable params\n",
       "22.7 M    Total params\n",
       "90.854    Total estimated model params size (MB)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ModelSummary(lit_model, max_depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4144db97-0281-4d2c-a6c5-c391a9c5eea6",
   "metadata": {},
   "source": [
    "TODO..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a623613f-cd1c-401c-814c-a835b4bb5819",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "results_base = f\"../experiments/results/{config_id}\"\n",
    "csv_logger = CSVLogger(results_base, \"finetune_lightning_logs\")\n",
    "tb_logger = TensorBoardLogger(results_base, name=\"finetune_tb_logs\")\n",
    "finetune_model_checkpointer = ModelCheckpoint(\n",
    "    f\"{results_base}/finetune_checkpoints/version_{csv_logger.version}\",\n",
    "    filename=\"finetune_checkpoint\",\n",
    "    monitor=\"avg_val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=1\n",
    ")\n",
    "\n",
    "loggers = [csv_logger, tb_logger]\n",
    "callbacks = [finetune_model_checkpointer, RichProgressBar()]\n",
    "\n",
    "if finetune_model_checkpoint_path is not None:\n",
    "    checkpoint = torch.load(f\"../{finetune_model_checkpoint_path}\")\n",
    "    # Account for zero indexing\n",
    "    finetune_epochs = checkpoint[\"epoch\"] + 1\n",
    "\n",
    "finetune_trainer = pl.Trainer(\n",
    "    max_epochs=finetune_epochs,\n",
    "    accelerator=accelerator,\n",
    "    callbacks=callbacks,\n",
    "    precision=\"16-mixed\",\n",
    "    logger=loggers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ec0c2c9-8dd0-4432-b5cc-5942bb48816b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at ../experiments/results/mlml6_rate_pred_clsp/finetune_checkpoints/version_0/finetune_checkpoint.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from: experiments/results/mlml6_rate_pred_clsp/frozen_checkpoints/version_0/checkpoint.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name           </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type          </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ text_regressor │ TextRegressor │ 22.7 M │\n",
       "└───┴────────────────┴───────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName          \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType         \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ text_regressor │ TextRegressor │ 22.7 M │\n",
       "└───┴────────────────┴───────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 22.7 M                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 22.7 M                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 90                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 22.7 M                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 22.7 M                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 90                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restored all states from the checkpoint at ../experiments/results/mlml6_rate_pred_clsp/finetune_checkpoints/version_0/finetune_checkpoint.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "937756d8fa0041c0a9dcc42e5750a1f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=4` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if finetune_model_checkpoint_path is not None:\n",
    "    print(f\"Loading checkpoint from: {frozen_model_checkpoint_path}\")\n",
    "    finetune_trainer.fit(\n",
    "        model=lit_model,\n",
    "        ckpt_path=f\"../{finetune_model_checkpoint_path}\",\n",
    "        train_dataloaders=train_dataloader,\n",
    "        val_dataloaders=valid_dataloader\n",
    "    )\n",
    "else:\n",
    "    print(f\"Training from scratch\")\n",
    "    finetune_trainer.fit(\n",
    "        model=lit_model,\n",
    "        train_dataloaders=train_dataloader,\n",
    "        val_dataloaders=valid_dataloader\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c3f0d68-b124-4071-865a-641284686587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/carcook/dev/nlp-projects/experiments/results/mlml6_rate_pred_clsp/finetune_checkpoints/version_0/finetune_checkpoint.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /home/carcook/dev/nlp-projects/experiments/results/mlml6_rate_pred_clsp/finetune_checkpoints/version_0/finetune_checkpoint.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7668ff9e8e734f0490f8316787d852cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       avg_test_loss       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8175450563430786     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      avg_test_loss      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8175450563430786    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'avg_test_loss': 0.8175450563430786}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_ft_checkpoint_path = finetune_trainer.checkpoint_callback.best_model_path\n",
    "finetune_trainer.test(lit_model, dataloaders=valid_dataloader, ckpt_path=best_ft_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50d52b4f-b12a-4f53-a20e-8c0f45c196b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mse': 0.8175451, 'rmse': 0.904182}\n"
     ]
    }
   ],
   "source": [
    "print(lit_model.test_epoch_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b24f3b86-4fdb-4074-8437-1505af637a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/carcook/dev/nlp-projects/experiments/results/mlml6_rate_pred_clsp/finetune_checkpoints/version_0/finetune_checkpoint.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /home/carcook/dev/nlp-projects/experiments/results/mlml6_rate_pred_clsp/finetune_checkpoints/version_0/finetune_checkpoint.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "782c8970722b43408ed63eb2ec422e8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       avg_test_loss       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8960340619087219     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      avg_test_loss      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8960340619087219    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'avg_test_loss': 0.8960340619087219}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using test set instead of val set since val data were (somewhat)\n",
    "# used for model selection by helping make some design choices (e.g.,\n",
    "# pooling strategy) and in determining when overfitting occurred (e.g.,\n",
    "# deciding when to overwrite checkpoints).\n",
    "best_ft_checkpoint_path = finetune_trainer.checkpoint_callback.best_model_path\n",
    "finetune_trainer.test(lit_model, dataloaders=test_dataloader, ckpt_path=best_ft_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0447f646-8fd4-4d04-9e95-7cc13a47a54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mse': 0.8960342, 'rmse': 0.94659084}\n"
     ]
    }
   ],
   "source": [
    "print(lit_model.test_epoch_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a428b55-463f-4407-a8fe-524ffd3389e4",
   "metadata": {},
   "source": [
    "[<< Previous: Data Exploration](02_data_explore.ipynb) &nbsp; | &nbsp; [Next: Fit Analysis >>](04_fit_analysis.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
