{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c533ea7b-b3c7-4a65-a4f8-83a21182e918",
   "metadata": {},
   "source": [
    "[<< Previous: Data Exploration](02_data_explore.ipynb) &nbsp; | &nbsp; [Next: Fit Analysis >>](04_fit_analysis.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dce0876-eeb9-4657-820f-8f84e0fa8e94",
   "metadata": {},
   "source": [
    "# Model Fitting\n",
    "\n",
    "In this notebook, we will perform model fitting and collect metrics for evaluating the performance of our fitted model.\n",
    "\n",
    "We are fine-tuning a [`MiniLM-L6`](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) model. This model generates embeddings that capture information about passages of text, and can be used for various NLP tasks. This model is much lighter-weight (i.e., has fewer parameters) than other transformer models well-suited to the same task, but still delivers good quality performance. A helpful comparison of model architecture performance can be found [here](https://www.sbert.net/docs/pretrained_models.html). This comparison was created by the authors of this (and several other) models uploaded to the [Hugging Face](https://huggingface.co/) model repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280fe7a3-718a-41a9-ae84-91ac0e58bc17",
   "metadata": {},
   "source": [
    "First, let's import the libraries that will be required for this notebook.\n",
    "\n",
    "We will use code from our custom package for this project, `myutilpy`. Specifically, we will utilize model implementations and utility functions from the `myutilpy.models` module. All of the source code is available in this repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19bf0c32-abf2-4170-8b46-93288e4fc514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import multiprocessing\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import lightning.pytorch as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from lightning.pytorch.callbacks import RichProgressBar, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import TensorBoardLogger, CSVLogger\n",
    "from lightning.pytorch.utilities.model_summary import ModelSummary\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from myutilpy.models.text_regressor import TextRegressor, LitTextRegressor\n",
    "from myutilpy.models.pooling import pooling_fns, pool_cls, pool_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5092940b-733c-4dd5-8df2-19ab3b985bca",
   "metadata": {},
   "source": [
    "Before we move on, let's silence a few unconsequential, known warning messages that will clutter our cell output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a233a63a-b9b9-43a4-845e-bcf963c1a3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*Detected KeyboardInterrupt, attempting graceful shutdown.*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*Only `best_model_path` will be reloaded.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989fb736-28e5-45c0-9ebf-be86c9c3ec6e",
   "metadata": {},
   "source": [
    "## Configurations\r\n",
    "\r\n",
    "Next, let’s do some setup. We will load the associated configurations for the desired experimenory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b868704-6b23-4a24-84a6-fe4077833f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_id = \"mlml6_rate_pred_clsp\"\n",
    "num_cores_avail = max(1, multiprocessing.cpu_count() - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2993eeb5-c0e8-4d4a-a1e2-72ff2042e572",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../experiments/configs/{config_id}/main.yaml\", 'r') as f:\n",
    "    main_config = yaml.safe_load(f)\n",
    "\n",
    "with open(f\"../experiments/configs/{config_id}/model.yaml\", 'r') as f:\n",
    "    model_config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "714dcbe9-12ca-468f-96d4-231667e559a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_checkpoint = main_config[\"dataset_checkpoint\"]\n",
    "dataset_checkpoint_revision = main_config[\"dataset_checkpoint_revision\"]\n",
    "pt_model_checkpoint = main_config[\"pt_model_checkpoint\"]\n",
    "pt_model_checkpoint_revision = main_config[\"pt_model_checkpoint_revision\"]\n",
    "dataset_id = main_config[\"dataset_id\"]\n",
    "frozen_model_checkpoint_path = model_config[\"frozen_model_checkpoint_path\"]\n",
    "finetune_model_checkpoint_path = model_config[\"finetune_model_checkpoint_path\"]\n",
    "model_seed = model_config[\"model_seed\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c493fef-aecc-4957-bb8b-6d9436ca0c92",
   "metadata": {},
   "source": [
    "## Base model, tokenizer, and dataset\n",
    "\n",
    "Let's load in our base embedding model, tokenizer, and preprocessed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d42f506-1222-4a62-bba7-7717ceb99f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = AutoModel.from_pretrained(\n",
    "    pt_model_checkpoint,\n",
    "    revision=pt_model_checkpoint_revision\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pt_model_checkpoint,\n",
    "    revision=pt_model_checkpoint_revision\n",
    ")\n",
    "\n",
    "datasets = load_from_disk(f\"../data/pitchfork/{dataset_id}/dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72467087-5a5e-4565-9c2f-9593441d9a8c",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "We only need a subset of the dataset columns for our model-fitting. Our tokenizer will output the `\"input_ids\"` and `\"attention_mask\"` columns. Strictly speaking, we only really need the `\"input_ids\"`, `\"attention_mask\"`, and `\"rating\"` columns, but keeping additional columns can be helpful for debugging and development purposes when performance is not a key concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61e5339d-c556-45e5-b547-6916170f6fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "keeper_cols = [\"artist\", \"album\", \"year_released\", \"rating\", \"input_ids\", \"attention_mask\"]\n",
    "drop_cols = set(datasets[\"train\"].column_names).difference(set(keeper_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "993f416c-d209-47e3-a701-f33082f1857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = (\n",
    "    datasets\n",
    "        .map(lambda examples: tokenizer(examples[\"review\"], padding=True, truncation=True), batched=True, num_proc=num_cores_avail)\n",
    "        .remove_columns(drop_cols)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ccd385-8f9b-421d-b70c-757df8f44661",
   "metadata": {},
   "source": [
    "### DataLoaders\r\n",
    "\r\n",
    "Let's set up the `DataLoader` objects that will be used for fitting and evaluating our mode.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1424a9b1-2395-4c7e-b3a1-eae07670679f",
   "metadata": {},
   "source": [
    "Our first task will be to define a collation function whose job it is to organize our batched examples into tensors that are ready to be passed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2d26608-f5da-4be4-bfa4-f8c384ed2fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_reviews(batch):\n",
    "    # Extract input_ids and labels from the batch\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_masks = [item['attention_mask'] for item in batch]\n",
    "    ratings = [item['rating'] for item in batch]\n",
    "\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "    ratings = torch.tensor(ratings)\n",
    "\n",
    "    return input_ids, attention_masks, ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437dbd76-3615-4380-9db5-359de24d4aad",
   "metadata": {},
   "source": [
    "Next, let's instantiate the `DataLoader` objects. Notice that we have some commented-out code. We also define our batch size, since this is required to instantiate a `DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "749908e3-02d6-4aa8-9834-5379af5959ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f109e655-fe2e-42a9-bb6f-c5c84a0fb80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], batch_size=batch_size, collate_fn=collate_reviews, shuffle=True)\n",
    "valid_dataloader = DataLoader(tokenized_datasets[\"validation\"], batch_size=batch_size, collate_fn=collate_reviews)\n",
    "test_dataloader = DataLoader(tokenized_datasets[\"test\"], batch_size=batch_size, collate_fn=collate_reviews)\n",
    "\n",
    "# # Random subsets for quick development\n",
    "# train_dataloader = DataLoader(tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1500)), batch_size=batch_size, collate_fn=collate_reviews, shuffle=True)\n",
    "# valid_dataloader = DataLoader(tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(1500)), batch_size=batch_size, collate_fn=collate_reviews)\n",
    "# test_dataloader = DataLoader(tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1500)), batch_size=batch_size, collate_fn=collate_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581f7860-a17a-4063-8819-70365d024128",
   "metadata": {},
   "source": [
    "Before moving on, let's do a quick spot check to make sure that our forward pass and pooling code are outputting tensors of correct dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1785ce37-73af-4261-81c8-8ec4e63c8a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, batch in enumerate(valid_dataloader):\n",
    "    input_ids, attention_masks, ratings = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6929f59d-2b5d-4c71-9daf-c629ccbce19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    embedding = embedding_model(input_ids=input_ids, attention_mask=attention_masks).last_hidden_state\n",
    "mp_embedding = pool_mean(embedding, attention_masks)\n",
    "cp_embedding = pool_cls(embedding, attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "607e4c9f-2cf1-4bfe-9df8-efe19d909028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 384\n",
      "16 384\n",
      "16 384\n"
     ]
    }
   ],
   "source": [
    "print(batch_size, embedding_model.config.hidden_size)\n",
    "print(*mp_embedding.shape)\n",
    "print(*cp_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37d13a1-be23-43a6-bb12-8cfc0317e3ce",
   "metadata": {},
   "source": [
    "Everything looks good in terms of dimensionality. Our pooled embeddings are outputting tensors of shape (batch size, embedding dimension)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd5498b-9244-4257-b08f-379977f06292",
   "metadata": {},
   "source": [
    "## Setup for fitting\r\n",
    "\r\n",
    "Now, let's prepare to fit our model. Notice that we have two epoch variablesWe are fine-tuning a pre-trained model and will follow the common pattern of freezing the base model's parameters and training the new model head, then un-freezing the base model's parameters and training the full model with a low learning rate.ters.\r\n",
    "\r\n",
    "Here, we will prelatively fewery few epochs of training in both stages for a couple of reasons. First, this code is not being executed in a cloud environment with large amounts of compute, so we will try and keep things manageable. Second, given the dataset size and the relatively small number of parameters in the regression model head, the first step likely does not need all that many epochs to obtain good performance. Finally, when fine-tuning the model, it is often the case that overfitting will occur within just a few epochs due to how powerful the base model will typically be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9c74bc-2cad-4313-a4bc-1a01d9070d57",
   "metadata": {},
   "source": [
    "Let's define our epoch counts and also define the device that our model will run on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9dbe54ec-f19e-41a1-ae57-27b9e36897b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "frozen_epochs = 20\n",
    "finetune_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173f681e-1ec2-4357-816a-dd9d493b1099",
   "metadata": {},
   "source": [
    "Let's also set a random seed for model fitting so that we can reproduce our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9176615b-cb46-4190-b1df-319478e53e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(seed=model_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01efb133-6a19-4761-91e8-53cfc06ddb0b",
   "metadata": {},
   "source": [
    "## Frozen fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1edbf9-0322-4e3c-b20f-4150e7923210",
   "metadata": {},
   "source": [
    "### Frozen trainer\r\n",
    "\r\n",
    "For this project, we will be using the [`Lightning`](https://lightning.ai/) library. It provides many helpful utilities and takes away a lot of the boilerplate programming work that can slow down model development in `PyTorch`. For details on how `Lightning` is used to help with our model fitting and evaluation code, check out the `myutilpy.models.text_regressor` module in the source code. For the purposes of this notebook, we only really need to set up the `Trainer` object. The trainer will handle model training and evaluation for us during the fitting process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0b5706-8b8f-477c-ae0c-90d78ebe3e50",
   "metadata": {},
   "source": [
    "The first thing we will take care of for setting up the trainer is to define our logging and checkpointing utilities. We will log metrics to both a `.csv` file and for `tensorboard` visualization. While the tensorboard output will not appear in this notebook's output, it is a very convenient tool for examining model behavior during fitting, and was used during the development of this notebook. We also want to make sure we retain checkpoints of our model so that we can load them in later. Here, we decide to keep only the checkpoint whose model parameters yielded the minimum validation loss out of all training epochs (i.e., the \"best\" version of the frozen model). Notice that we specify this checkpointer is only for the \"frozen\" training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d9cd192-3269-4541-a0fe-44f17e0150f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_base = f\"../experiments/results/{config_id}\"\n",
    "\n",
    "csv_logger = CSVLogger(results_base, \"frozen_lightning_logs\")\n",
    "tb_logger = TensorBoardLogger(results_base, name=\"frozen_tb_logs\")\n",
    "frozen_model_checkpointer = ModelCheckpoint(\n",
    "    f\"{results_base}/frozen_checkpoints/version_{csv_logger.version}\",\n",
    "    filename=\"checkpoint\",\n",
    "    monitor=\"avg_val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=1\n",
    ")\n",
    "\n",
    "loggers = [csv_logger, tb_logger]\n",
    "callbacks = [frozen_model_checkpointer, RichProgressBar()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3aec3f-fb6c-412a-8365-65ebc493e4d4",
   "metadata": {},
   "source": [
    "Our next step is to instantiate our frozen `Trainer` object. Notice that we also make a (potential) modification to the number of frozen training epochs. The reason for this is that a saved checkpoint will contain the epoch at which the checkpoint was written out. Since this notebook was already run (behind the scenes, like a cooking show where all of the ingredients are always magically prepped!) and the best version of the frozen model was already saved out, we don't want to start training where we left off if the best version of the model was found before the final epoch. This would be the case if overfitting began to occur before the final epoch. So, we update our `max_epochs` to reflect the number of epochs it took to find the best frozen model (behind the scenes) and this means the trainer will not attempt to run additional fitting epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13f80ce5-0fcf-4704-b35a-6ef5d423a0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "if frozen_model_checkpoint_path is not None:\n",
    "    checkpoint = torch.load(f\"../{frozen_model_checkpoint_path}\")\n",
    "    # Account for zero indexing\n",
    "    frozen_epochs = checkpoint[\"epoch\"] + 1\n",
    "\n",
    "frozen_trainer = pl.Trainer(\n",
    "    max_epochs=frozen_epochs,\n",
    "    accelerator=accelerator,\n",
    "    callbacks=callbacks,\n",
    "    precision=\"16-mixed\",\n",
    "    logger=loggers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdd1cf7-40e7-4c78-bd40-42d3bb5bb59f",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Let's now instantiate our full model (base model + regression head) using the `TextRegressor` class defined in `myutilpy.models.text_regressor`.\n",
    "\n",
    "Note that the pooling method we use for this experiment is \"CLS pooling\" (pooling by extracting the starting `\"[CLS]\"`) token. This is a popular method when using BERT-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce37ecf8-030c-4e97-b49c-6917e4732070",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_model = TextRegressor(\n",
    "    embedding_model,\n",
    "    embed_dim=embedding_model.config.hidden_size,\n",
    "    pooling_fn=pooling_fns[model_config[\"pooling\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7986462e-ab29-4a34-96ac-dcbabdcdeb06",
   "metadata": {},
   "source": [
    "### Freeze parameters\n",
    "\n",
    "The final step is to instantiate our `Lightning` model `LitTextRegressor` which wraps around our `TextRegressor` model and freeze the base model parameters. A helper method `LitTextRegressor.freeze_pretrained_model()` has been implemented to freeze the base model parameters and set the learning rate appropriately for frozen training. For full implementation details, see `myutilpy.models.text_regressor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea3f7f87-1f08-4f9f-b8a3-803786c0c4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model = LitTextRegressor(tr_model)\n",
    "lit_model.freeze_pretrained_model(lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558d2f7e-7690-4459-8134-a2b70fe64561",
   "metadata": {},
   "source": [
    "Let's verify that the base model parameters are indeed frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4220bf6a-4fc5-4aea-8679-a1f9aa416e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  | Name                           | Type          | Params\n",
       "-----------------------------------------------------------------\n",
       "0 | text_regressor                 | TextRegressor | 22.7 M\n",
       "1 | text_regressor.embedder        | BertModel     | 22.7 M\n",
       "2 | text_regressor.regression_head | Linear        | 385   \n",
       "-----------------------------------------------------------------\n",
       "385       Trainable params\n",
       "22.7 M    Non-trainable params\n",
       "22.7 M    Total params\n",
       "90.854    Total estimated model params size (MB)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ModelSummary(lit_model, max_depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf71bf6-b881-44c5-9151-d6c8c6a35997",
   "metadata": {},
   "source": [
    "### Fitting the frozen model\n",
    "\n",
    "Finally, we fit our model with base parameters frozen. Again, because the model was already fit behind the scenes, a checkpoint path is used. Apologies for being denied the satisfaction of observing training bars that have reached completion &#x1F641;."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8da17839-011e-478c-9ecd-9bfb3ea22269",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at ../experiments/results/mlml6_rate_pred_clsp/frozen_checkpoints/version_0/checkpoint.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from: experiments/results/mlml6_rate_pred_clsp/frozen_checkpoints/version_0/checkpoint.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name           </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type          </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ text_regressor │ TextRegressor │ 22.7 M │\n",
       "└───┴────────────────┴───────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName          \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType         \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ text_regressor │ TextRegressor │ 22.7 M │\n",
       "└───┴────────────────┴───────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 385                                                                                              \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 22.7 M                                                                                       \n",
       "<span style=\"font-weight: bold\">Total params</span>: 22.7 M                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 90                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 385                                                                                              \n",
       "\u001b[1mNon-trainable params\u001b[0m: 22.7 M                                                                                       \n",
       "\u001b[1mTotal params\u001b[0m: 22.7 M                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 90                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restored all states from the checkpoint at ../experiments/results/mlml6_rate_pred_clsp/frozen_checkpoints/version_0/checkpoint.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9974a961eee94053af326a60d161aba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if frozen_model_checkpoint_path is not None:\n",
    "    print(f\"Loading checkpoint from: {frozen_model_checkpoint_path}\")\n",
    "    frozen_trainer.fit(\n",
    "        model=lit_model,\n",
    "        ckpt_path=f\"../{frozen_model_checkpoint_path}\",\n",
    "        train_dataloaders=train_dataloader,\n",
    "        val_dataloaders=valid_dataloader\n",
    "    )\n",
    "else:\n",
    "    print(f\"Training from scratch\")\n",
    "    frozen_trainer.fit(\n",
    "        model=lit_model,\n",
    "        train_dataloaders=train_dataloader,\n",
    "        val_dataloaders=valid_dataloader\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9bb178-525b-4c5e-bd6a-ef3891d08f24",
   "metadata": {},
   "source": [
    "### Validation data check\n",
    "\n",
    "Before we move on to the next stage, let's see how well the best version of our frozen model performs on our validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44c549b7-7e01-4c97-b757-4f9598d53664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/carcook/dev/nlp-projects/experiments/results/mlml6_rate_pred_clsp/frozen_checkpoints/version_0/checkpoint.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /home/carcook/dev/nlp-projects/experiments/results/mlml6_rate_pred_clsp/frozen_checkpoints/version_0/checkpoint.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1080362fb2ae4f8088f931e11ecbd763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/carcook/dev/nlp-projects/myutilpy/myutilpy/models/text_regressor.py:136: UserWarning: Using a target size \n",
       "(torch.Size([16, 1])) that is different to the input size (torch.Size([16])). This will likely lead to incorrect \n",
       "results due to broadcasting. Please ensure they have the same size.\n",
       "  loss = self.criterion(yhat, ratings.unsqueeze(1))\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/carcook/dev/nlp-projects/myutilpy/myutilpy/models/text_regressor.py:136: UserWarning: Using a target size \n",
       "(torch.Size([16, 1])) that is different to the input size (torch.Size([16])). This will likely lead to incorrect \n",
       "results due to broadcasting. Please ensure they have the same size.\n",
       "  loss = self.criterion(yhat, ratings.unsqueeze(1))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/carcook/dev/nlp-projects/myutilpy/myutilpy/models/text_regressor.py:136: UserWarning: Using a target size \n",
       "(torch.Size([13, 1])) that is different to the input size (torch.Size([13])). This will likely lead to incorrect \n",
       "results due to broadcasting. Please ensure they have the same size.\n",
       "  loss = self.criterion(yhat, ratings.unsqueeze(1))\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/carcook/dev/nlp-projects/myutilpy/myutilpy/models/text_regressor.py:136: UserWarning: Using a target size \n",
       "(torch.Size([13, 1])) that is different to the input size (torch.Size([13])). This will likely lead to incorrect \n",
       "results due to broadcasting. Please ensure they have the same size.\n",
       "  loss = self.criterion(yhat, ratings.unsqueeze(1))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       avg_test_loss       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.4109914302825928     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      avg_test_loss      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.4109914302825928    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'avg_test_loss': 1.4109914302825928}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_frozen_checkpoint_path = frozen_trainer.checkpoint_callback.best_model_path\n",
    "frozen_trainer.test(lit_model, dataloaders=valid_dataloader, ckpt_path=best_frozen_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6fa35286-4179-45dd-810e-b4be84abc93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mse': 1.1630003, 'rmse': 1.0784249}\n"
     ]
    }
   ],
   "source": [
    "print(lit_model.test_epoch_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b39a068-0f14-46be-bbb9-94f60801d350",
   "metadata": {},
   "source": [
    "Pretty good! Using a fully frozen pre-trained base model, we have trained a regression model head to predict ratings to within +/- 1.1 rating \"points\" on average in our validation set, based only on review text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d61b36-2ea3-4022-9a95-7ea2dcbf6613",
   "metadata": {},
   "source": [
    "## Un-frozen fitting (fine-tuning)\r\n",
    "\r\n",
    "It is now time for the second stage, where we un-freeze our base model's parameters and perform fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6be3a3-896c-4aec-8e29-c74540b6ae79",
   "metadata": {},
   "source": [
    "Before we move on, let's clear up some memory on our GPU (if applicable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28e3797f-dd59-4446-a81b-a1dce8fa6e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free GPU memory\n",
    "del lit_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab08990-4d1c-4656-a391-62aee56d0a4e",
   "metadata": {},
   "source": [
    "### Load in best frozen model\r\n",
    "\r\n",
    "Let's load in the best checkpoint for our frozen model and use it in the fine-tuning stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "278f34ab-4c43-4647-9107-91abaa5e2936",
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model = LitTextRegressor.load_from_checkpoint(\n",
    "    frozen_trainer.checkpoint_callback.best_model_path,\n",
    "    text_regressor = tr_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc00a9e-324b-4be2-9f64-e9dda389c6fa",
   "metadata": {},
   "source": [
    "Let's now un-freeze the base model's parameters (and set a lower learning rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "03e71598-93db-4717-a02b-042f9a3cd35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model.unfreeze_pretrained_model(1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2163f39c-5502-44b4-b7b7-04c9594c02a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  | Name                           | Type          | Params\n",
       "-----------------------------------------------------------------\n",
       "0 | text_regressor                 | TextRegressor | 22.7 M\n",
       "1 | text_regressor.embedder        | BertModel     | 22.7 M\n",
       "2 | text_regressor.regression_head | Linear        | 385   \n",
       "-----------------------------------------------------------------\n",
       "22.7 M    Trainable params\n",
       "0         Non-trainable params\n",
       "22.7 M    Total params\n",
       "90.854    Total estimated model params size (MB)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ModelSummary(lit_model, max_depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4144db97-0281-4d2c-a6c5-c391a9c5eea6",
   "metadata": {},
   "source": [
    "Now, we instantiate our trainer, loggers, and callbacks as in the frozen training section above. Again, we update our maximum epochs argument to be the number of epochs it took to find the best model parameters when the checkpoint was saved out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a623613f-cd1c-401c-814c-a835b4bb5819",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "results_base = f\"../experiments/results/{config_id}\"\n",
    "csv_logger = CSVLogger(results_base, \"finetune_lightning_logs\")\n",
    "tb_logger = TensorBoardLogger(results_base, name=\"finetune_tb_logs\")\n",
    "finetune_model_checkpointer = ModelCheckpoint(\n",
    "    f\"{results_base}/finetune_checkpoints/version_{csv_logger.version}\",\n",
    "    filename=\"finetune_checkpoint\",\n",
    "    monitor=\"avg_val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=1\n",
    ")\n",
    "\n",
    "loggers = [csv_logger, tb_logger]\n",
    "callbacks = [finetune_model_checkpointer, RichProgressBar()]\n",
    "\n",
    "if finetune_model_checkpoint_path is not None:\n",
    "    checkpoint = torch.load(f\"../{finetune_model_checkpoint_path}\")\n",
    "    # Account for zero indexing\n",
    "    finetune_epochs = checkpoint[\"epoch\"] + 1\n",
    "\n",
    "finetune_trainer = pl.Trainer(\n",
    "    max_epochs=finetune_epochs,\n",
    "    accelerator=accelerator,\n",
    "    callbacks=callbacks,\n",
    "    precision=\"16-mixed\",\n",
    "    logger=loggers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c038aa-ea22-45e6-9316-66776245aaa5",
   "metadata": {},
   "source": [
    "Finally, we perform a fine-tuning fit. Again, since a behind-the-scenes fit has already taken place and we are using a checkpoint, no progress bars or current epoch will show up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ec0c2c9-8dd0-4432-b5cc-5942bb48816b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at ../experiments/results/mlml6_rate_pred_clsp/finetune_checkpoints/version_0/finetune_checkpoint.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from: experiments/results/mlml6_rate_pred_clsp/frozen_checkpoints/version_0/checkpoint.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name           </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type          </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ text_regressor │ TextRegressor │ 22.7 M │\n",
       "└───┴────────────────┴───────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName          \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType         \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ text_regressor │ TextRegressor │ 22.7 M │\n",
       "└───┴────────────────┴───────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 22.7 M                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 22.7 M                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 90                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 22.7 M                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 22.7 M                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 90                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restored all states from the checkpoint at ../experiments/results/mlml6_rate_pred_clsp/finetune_checkpoints/version_0/finetune_checkpoint.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c512c4bec4c4102bec7ee1f672bb23a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=4` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if finetune_model_checkpoint_path is not None:\n",
    "    print(f\"Loading checkpoint from: {frozen_model_checkpoint_path}\")\n",
    "    finetune_trainer.fit(\n",
    "        model=lit_model,\n",
    "        ckpt_path=f\"../{finetune_model_checkpoint_path}\",\n",
    "        train_dataloaders=train_dataloader,\n",
    "        val_dataloaders=valid_dataloader\n",
    "    )\n",
    "else:\n",
    "    print(f\"Training from scratch\")\n",
    "    finetune_trainer.fit(\n",
    "        model=lit_model,\n",
    "        train_dataloaders=train_dataloader,\n",
    "        val_dataloaders=valid_dataloader\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc56a4e-15f6-4c37-9689-a887242a4d7e",
   "metadata": {},
   "source": [
    "### Validation data check\n",
    "\n",
    "Let's compare the performance of our fine-tuned model to that of our frozen model on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c3f0d68-b124-4071-865a-641284686587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/carcook/dev/nlp-projects/experiments/results/mlml6_rate_pred_clsp/finetune_checkpoints/version_0/finetune_checkpoint.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /home/carcook/dev/nlp-projects/experiments/results/mlml6_rate_pred_clsp/finetune_checkpoints/version_0/finetune_checkpoint.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6906ff407c5497c885c9328dc7df5ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/carcook/dev/nlp-projects/myutilpy/myutilpy/models/text_regressor.py:136: UserWarning: Using a target size \n",
       "(torch.Size([16, 1])) that is different to the input size (torch.Size([16])). This will likely lead to incorrect \n",
       "results due to broadcasting. Please ensure they have the same size.\n",
       "  loss = self.criterion(yhat, ratings.unsqueeze(1))\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/carcook/dev/nlp-projects/myutilpy/myutilpy/models/text_regressor.py:136: UserWarning: Using a target size \n",
       "(torch.Size([16, 1])) that is different to the input size (torch.Size([16])). This will likely lead to incorrect \n",
       "results due to broadcasting. Please ensure they have the same size.\n",
       "  loss = self.criterion(yhat, ratings.unsqueeze(1))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/carcook/dev/nlp-projects/myutilpy/myutilpy/models/text_regressor.py:136: UserWarning: Using a target size \n",
       "(torch.Size([13, 1])) that is different to the input size (torch.Size([13])). This will likely lead to incorrect \n",
       "results due to broadcasting. Please ensure they have the same size.\n",
       "  loss = self.criterion(yhat, ratings.unsqueeze(1))\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/carcook/dev/nlp-projects/myutilpy/myutilpy/models/text_regressor.py:136: UserWarning: Using a target size \n",
       "(torch.Size([13, 1])) that is different to the input size (torch.Size([13])). This will likely lead to incorrect \n",
       "results due to broadcasting. Please ensure they have the same size.\n",
       "  loss = self.criterion(yhat, ratings.unsqueeze(1))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       avg_test_loss       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.781655192375183     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      avg_test_loss      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.781655192375183    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'avg_test_loss': 1.781655192375183}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_ft_checkpoint_path = finetune_trainer.checkpoint_callback.best_model_path\n",
    "finetune_trainer.test(lit_model, dataloaders=valid_dataloader, ckpt_path=best_ft_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50d52b4f-b12a-4f53-a20e-8c0f45c196b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mse': 0.8175451, 'rmse': 0.904182}\n"
     ]
    }
   ],
   "source": [
    "print(lit_model.test_epoch_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b6add8-dc1d-4aad-b9c8-34fcdf8d5d67",
   "metadata": {},
   "source": [
    "It looks like fine-tuning definitely yielded an improvement!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beafa88-0b1b-40a0-9237-9ee80d8d4f8a",
   "metadata": {},
   "source": [
    "### Test data check\n",
    "\n",
    "To wrap things up, let's take a look at how our fine-tuned model performs on a completely held-out test set. Since we used the validation data to decide which checkpoint to retain for both the frozen and fine-tuning fitting, it is better to use an entirely held-out set of examples to assess estimated model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b24f3b86-4fdb-4074-8437-1505af637a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/carcook/dev/nlp-projects/experiments/results/mlml6_rate_pred_clsp/finetune_checkpoints/version_0/finetune_checkpoint.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /home/carcook/dev/nlp-projects/experiments/results/mlml6_rate_pred_clsp/finetune_checkpoints/version_0/finetune_checkpoint.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad7a5a1868a4089aaeca24f80734adb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/carcook/dev/nlp-projects/myutilpy/myutilpy/models/text_regressor.py:136: UserWarning: Using a target size \n",
       "(torch.Size([16, 1])) that is different to the input size (torch.Size([16])). This will likely lead to incorrect \n",
       "results due to broadcasting. Please ensure they have the same size.\n",
       "  loss = self.criterion(yhat, ratings.unsqueeze(1))\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/carcook/dev/nlp-projects/myutilpy/myutilpy/models/text_regressor.py:136: UserWarning: Using a target size \n",
       "(torch.Size([16, 1])) that is different to the input size (torch.Size([16])). This will likely lead to incorrect \n",
       "results due to broadcasting. Please ensure they have the same size.\n",
       "  loss = self.criterion(yhat, ratings.unsqueeze(1))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/carcook/dev/nlp-projects/myutilpy/myutilpy/models/text_regressor.py:136: UserWarning: Using a target size \n",
       "(torch.Size([14, 1])) that is different to the input size (torch.Size([14])). This will likely lead to incorrect \n",
       "results due to broadcasting. Please ensure they have the same size.\n",
       "  loss = self.criterion(yhat, ratings.unsqueeze(1))\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/carcook/dev/nlp-projects/myutilpy/myutilpy/models/text_regressor.py:136: UserWarning: Using a target size \n",
       "(torch.Size([14, 1])) that is different to the input size (torch.Size([14])). This will likely lead to incorrect \n",
       "results due to broadcasting. Please ensure they have the same size.\n",
       "  loss = self.criterion(yhat, ratings.unsqueeze(1))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       avg_test_loss       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.8949449062347412     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      avg_test_loss      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.8949449062347412    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'avg_test_loss': 1.8949449062347412}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_ft_checkpoint_path = finetune_trainer.checkpoint_callback.best_model_path\n",
    "finetune_trainer.test(lit_model, dataloaders=test_dataloader, ckpt_path=best_ft_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0447f646-8fd4-4d04-9e95-7cc13a47a54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mse': 0.8960342, 'rmse': 0.94659084}\n"
     ]
    }
   ],
   "source": [
    "print(lit_model.test_epoch_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b87fda-b968-48a6-94bb-645d91b49fa1",
   "metadata": {},
   "source": [
    "Still pretty good! While this is slightly higher RMSE than what was achieved on the validation set, it is still rather good when you consider the difficulty of the prediction task and the relative simplicity of the model head."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb83f1b-789e-4195-b9b7-5f6c3647b070",
   "metadata": {},
   "source": [
    "Finally, let's save our results for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4eaf8663-34ec-4fff-9719-1052ea0c43e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(\n",
    "    data={\n",
    "        \"y\": torch.concat(lit_model.test_epoch_out[\"y\"]).to(\"cpu\").numpy(),\n",
    "        \"yhat\": torch.concat(lit_model.test_epoch_out[\"yhat\"]).to(\"cpu\").numpy()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2b2cde34-85c7-4a36-ba79-5c067f4f40e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv(f\"{results_base}/predictions_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a428b55-463f-4407-a8fe-524ffd3389e4",
   "metadata": {},
   "source": [
    "[<< Previous: Data Exploration](02_data_explore.ipynb) &nbsp; | &nbsp; [Next: Fit Analysis >>](04_fit_analysis.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
